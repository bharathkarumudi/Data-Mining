{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project:  SMS Spam filter Classifier\n",
    "\n",
    "*Submitted By: Bharath Karumudi.*\n",
    "\n",
    "For this assignment I have selected the dataset \"SMS Spam Collection\" from Kaggle: https://www.kaggle.com/uciml/sms-spam-collection-dataset  and will be performing a classification problem by comparing with different models and see which model performed bettter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Building a Spam Classifier\n",
    "\n",
    "We can build a Spam classifier by using the probability theory - Baye's theorem, which we can express as:\n",
    "P(A|B) = P(B|A) * P(A) / P(B). \n",
    "We’ll train our filter using a collection of spam and non-spam(aka “ham”) SMS. So, we’ll provide right answers to train the filter, and later in the prediction phase, its output for a given message would be either “spam” or “ham”. So, this filter is an example of a supervised classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several models are built to predit the SMS Spam classifier and the framework follows as below:\n",
    "<img src=\"model-framework.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additonal work included:\n",
    "1. Added lemmantization\n",
    "2. added feature avg. len of words in sms\n",
    "3. added feature len of sms\n",
    "4. added frequency of capitals in the sms.\n",
    "5. Applied StratifiedCV in GridSearch for internal CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/bharathkarumudi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import accuracy_score,f1_score, confusion_matrix, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr \n",
    "import xgboost as xgb\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. About Dataset\n",
    "The dataset is a SMS Spam collection that was collected for SMS Spam research and have 5,574 English messages and tagged as ham or spam.\n",
    "The file contains, two columns: V1 contains the label and V2 contains the SMS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inspecting the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset\n",
    "Let's load the dataset which is a csv file and then visualize how it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('data/spam.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the dataset: 5572\n",
      "\n",
      "Let's see sample records from dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Number of records in the dataset: \" + str(dat.shape[0]))\n",
    "print (\"\\nLet's see sample records from dataset:\")\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      "v1            5572 non-null object\n",
      "v2            5572 non-null object\n",
      "Unnamed: 2    50 non-null object\n",
      "Unnamed: 3    12 non-null object\n",
      "Unnamed: 4    6 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.7+ KB\n"
     ]
    }
   ],
   "source": [
    "dat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Number of ham and spam messages')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Data Distribution:\")\n",
    "sns.countplot(dat.v1)\n",
    "plt.xlabel('Label')\n",
    "plt.title('Number of ham and spam messages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can see, we have a corpus of 5,572 records written in English and also has a target variable in first column which tells whether the message is a Spam or Ham and the second column is a text message which is in English.\n",
    "\n",
    "The last three are NaN which are not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are far fewer training examples for spam than ham—we'll take this imbalance into account in the analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rename the columns as 'Y' and 'SMS' and replacing the ham with 0 and Spam with 1 and dropping the columns that are not required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y                                                sms\n",
       "0  0  Go until jurong point, crazy.. Available only ...\n",
       "1  0                      Ok lar... Joking wif u oni...\n",
       "2  1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3  0  U dun say so early hor... U c already then say...\n",
       "4  0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = dat.loc[:, ['v1', 'v2']]\n",
    "dat.rename(columns={'v1': 'y', 'v2': 'sms'}, inplace=True)\n",
    "dat.y = dat.y.replace({'ham': 0, 'spam': 1})\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_txt(raw_text):\n",
    "    \n",
    "    processed = raw_text.str.replace(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr')\n",
    "    processed = processed.str.replace(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr')\n",
    "    processed = processed.str.replace(r'£|\\$', 'moneysymb')    \n",
    "    processed = processed.str.replace(r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', 'phonenumbr')\n",
    "    processed = processed.str.replace(r'\\d+(\\.\\d+)?', 'numbr')\n",
    "\n",
    "    processed = processed.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "    processed = processed.str.replace(r'\\s+', ' ')\n",
    "    processed = processed.str.replace(r'^\\s+|\\s+?$', '')\n",
    "\n",
    "    processed = processed.str.lower()\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = preprocess_txt(dat.sms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the stop words which do not add value, create ngrams and also filter the words that appears in almost all or very less apperance. \n",
    "\n",
    "Some words in the English language, while necessary, don't contribute much to the meaning of a phrase. These words, such as \"when\", \"had\", \"those\" or \"before\", are called stop words and should be filtered out. \n",
    "\n",
    "We can tokenize individual terms and generate what's called a bag of words model. You may notice this model has a glaring pitfall: it fails to capture the innate structure of human language. Under this model, the following sentences have the same feature vector although they convey dramatically different meanings.\n",
    "\n",
    "    Does steak taste delicious?\n",
    "    Steak does taste delicious.\n",
    "\n",
    "Alternatively, we can tokenize every sequence of n\n",
    "terms called n-grams. For example, tokenizing adjacent pairs of words yields bigrams. The n\n",
    "\n",
    "-gram model preserves word order and can potentially capture more information than the bag of words model.\n",
    "\n",
    "To get the best of both worlds, let's tokenize unigrams and bigrams. As an example, unigrams and bigrams for \"The quick brown fox\" are \"The\", \"quick\", \"brown\", \"fox\", \"The quick\", \"quick brown\" and \"brown fox\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode words\n",
    "vec = TfidfVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                      stop_words='english', # Remove stop words like a, an, the, etc that do not add much value.\n",
    "                      ngram_range=(1, 2),   # create unigrams and bigrams.\n",
    "                      min_df=0.01,          # filter words that appear in less than 1% of records\n",
    "                      max_df=0.99)          # filter words that appear in more than 99% of records.\n",
    "\n",
    "X = vec.fit_transform(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_len = dat.sms.apply(len).values\n",
    "message_len = message_len / max(message_len)\n",
    "\n",
    "message_n_words = np.array([len(x) for x in dat.sms.str.split()])\n",
    "message_n_words = message_n_words / max(message_n_words)\n",
    "\n",
    "all_caps_freq = np.array([sum(1 for c in message if c.isupper()) for message in dat.sms]) / message_len\n",
    "\n",
    "avg_word_len = []\n",
    "for message in dat.sms.str.split():\n",
    "    message_word_lens = []\n",
    "    for word in message:\n",
    "        message_word_lens.append(len(word))\n",
    "    avg_word_len.append(np.mean(message_word_lens))\n",
    "avg_word_len = np.array(avg_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coeff of 0.973724569719385 is greater than 0.9.\n",
      "So we will only use count of words in a message feature.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corr_coef, p = pearsonr(message_len, message_n_words) # highly correlated\n",
    "\n",
    "print('Correlation coeff of {0} is greater than 0.9.'.format(corr_coef))\n",
    "print('So we will only use count of words in a message feature.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack((X.todense(), message_n_words, all_caps_freq, avg_word_len))\n",
    "X_column_names = vec.get_feature_names() + ['message_n_words', 'all_caps_freq', 'avg_word_len']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Creating the train and test datasets:\n",
    "Using stratification on target variable creating 90% train and 10% test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, dat.y, test_size=0.1, stratify=dat.y, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Building the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be training the model with range of parameters and from \n",
    "# there we can get the best model out of it.\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "\n",
    "grid_search_svm = GridSearchCV(SVC(kernel='rbf', random_state=1234, class_weight='balanced'), \n",
    "                               param_grid,\n",
    "                               scoring=make_scorer(f1_score),\n",
    "                               cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=1234)\n",
    "                              ).fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see the model score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9587813620071685"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_svm.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using the best model\n",
    "svm_y_pred = grid_search_svm.best_estimator_.predict(X_test)\n",
    "\n",
    "# Build Confusion Matrix\n",
    "svm_cfm = pd.DataFrame(\n",
    "    metrics.confusion_matrix(y_test, svm_y_pred),\n",
    "    index=[['actual', 'actual'], ['spam', 'ham']],\n",
    "    columns=[['predicted', 'predicted'], ['spam', 'ham']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be training the model with range of parameters and from \n",
    "# there we can get the best model out of it.\n",
    "\n",
    "n_estimators = [100, 300, 500, 800, 1200]\n",
    "max_depth = [5, 8, 15, 25, 30]\n",
    "min_samples_split = [2, 5, 10, 15, 100]\n",
    "min_samples_leaf = [1, 2, 5, 10] \n",
    "\n",
    "param_grid = dict(n_estimators = n_estimators, \n",
    "              max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "              min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=1234, class_weight='balanced'), \n",
    "                     param_grid, \n",
    "                     cv = 3, \n",
    "                     scoring=make_scorer(f1_score),\n",
    "                     n_jobs = -1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see the model score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9767025089605734"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_rf.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using the best model\n",
    "rf_y_pred=grid_search_rf.best_estimator_.predict(X_test)\n",
    "\n",
    "# Build Confusion Matrix\n",
    "rf_cfm = pd.DataFrame(\n",
    "    metrics.confusion_matrix(y_test, rf_y_pred),\n",
    "    index=[['actual', 'actual'], ['spam', 'ham']],\n",
    "    columns=[['predicted', 'predicted'], ['spam', 'ham']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7570621468926553\n"
     ]
    }
   ],
   "source": [
    "cnb = ComplementNB(alpha=0.1).fit(X_train, y_train)\n",
    "nb_y_pred = cnb.predict(X_test)\n",
    "score = f1_score(y_pred=nb_y_pred, y_true=y_test, pos_label=1)\n",
    "    \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Confusion Matrix\n",
    "nb_cfm = pd.DataFrame(\n",
    "    metrics.confusion_matrix(y_test, nb_y_pred),\n",
    "    index=[['actual', 'actual'], ['spam', 'ham']],\n",
    "    columns=[['predicted', 'predicted'], ['spam', 'ham']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM: Accuracy=0.977, F1=0.909\n"
     ]
    }
   ],
   "source": [
    "data_tr  = xgb.DMatrix(X_train, label=y_train)\n",
    "data_val  = xgb.DMatrix(X_test, label=y_test)\n",
    "evallist = [(data_tr, 'train'), (data_val, 'test')]\n",
    "\n",
    "parms = {'max_depth':9,      #maximum depth of a tree\n",
    "         'objective':'binary:logistic',\n",
    "         'eta'      :0.1,\n",
    "         'subsample':0.8,    #SGD will use this percentage of data\n",
    "         'lambda '  :1.5,    #L2 regularization term, >1 more conservative\n",
    "         'colsample_bytree ':0.8,\n",
    "         'nthread'  :3}      #number of cpu core to use\n",
    "\n",
    "GBM = xgb.train(parms, data_tr, num_boost_round=118, evals = evallist,\n",
    "                maximize=False, verbose_eval=False)\n",
    "\n",
    "\n",
    "Acc = {}\n",
    "F1score = {}\n",
    "confusion_mat={}\n",
    "predictions = {}\n",
    "name = 'GBM'\n",
    "pred = GBM.predict(xgb.DMatrix(X_test))\n",
    "pred = [int(round(p)) for p in pred]\n",
    "F1score[name]= f1_score(y_test,pred)\n",
    "Acc[name] = accuracy_score(y_test,pred)\n",
    "confusion_mat[name] = confusion_matrix(y_test,pred)\n",
    "predictions[name]=pred\n",
    "print(name+': Accuracy=%1.3f, F1=%1.3f'%(Acc[name],F1score[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Confusion Matrix\n",
    "gb_cfm = pd.DataFrame(\n",
    "    metrics.confusion_matrix(y_test, pred),\n",
    "    index=[['actual', 'actual'], ['spam', 'ham']],\n",
    "    columns=[['predicted', 'predicted'], ['spam', 'ham']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have built models using different algorithms. Lets compare their performance and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for SVM:\n",
      "\n",
      "             predicted    \n",
      "                 spam ham\n",
      "actual spam       476   7\n",
      "       ham         16  59\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix for SVM:\\n\\n\", svm_cfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Random Forest:\n",
      "\n",
      "             predicted    \n",
      "                 spam ham\n",
      "actual spam       480   3\n",
      "       ham         10  65\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix for Random Forest:\\n\\n\", rf_cfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Naive Bayes:\n",
      "\n",
      "             predicted    \n",
      "                 spam ham\n",
      "actual spam       448  35\n",
      "       ham          8  67\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix for Naive Bayes:\\n\\n\", nb_cfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Gradient Boosting:\n",
      "\n",
      "             predicted    \n",
      "                 spam ham\n",
      "actual spam       480   3\n",
      "       ham         10  65\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix for Gradient Boosting:\\n\\n\", gb_cfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above confusion matrices, we can see the model that best suits is by Gradient Boosting. It has better score and better predictions compared to other models. Random Forest is also similar, but performanace wise it took time to build the model compared to the Gradient Boosting. The decisions are also depends on the business needs, because, in some cases, predicting a Spam as Ham will not impact much, but the other way around might impact, because there will be a loss of information. In some critical environment, where the security is higher, the False Negatives (predicting as Ham) is acceptable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Reference and work done:\n",
    "\n",
    "[1] https://inmachineswetrust.com/posts/sms-spam-filter/  \n",
    "[2] https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python\n",
    "\n",
    "Additonal work included:\n",
    "1. Added lemmantization  \n",
    "2. added feature avg. len of words in sms  \n",
    "3. added feature len of sms.  \n",
    "4. added frequency of capitals in the sms.  \n",
    "5. Applied StratifiedCV in GridSearch for internal CV.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
